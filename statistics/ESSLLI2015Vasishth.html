
<!DOCTYPE html>
<!--[if lt IE 7 ]><html class="ie ie6" lang="en"> <![endif]-->
<!--[if IE 7 ]><html class="ie ie7" lang="en"> <![endif]-->
<!--[if IE 8 ]><html class="ie ie8" lang="en"> <![endif]-->
<!--[if (gte IE 9)|!(IE)]><!--><html lang="en"> <!--<![endif]-->
<head>

	<!-- Basic Page Needs
  ================================================== -->
	<meta charset="utf-8">
	<title>Shravan Vasishth's home page</title>
	<meta name="description" content="SV's Home Page">
	<meta name="author" content="Shravan Vasishth">

	<!-- Mobile Specific Metas
  ================================================== -->
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

	<!-- CSS
  ================================================== -->
	<link rel="stylesheet" href="../stylesheets/base.css">
	<link rel="stylesheet" href="../stylesheets/skeleton.css">
	<link rel="stylesheet" href="../stylesheets/layout.css">

	<!--[if lt IE 9]>
		<script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
	<![endif]-->

	<!-- Favicons
	================================================== -->
	<link rel="shortcut icon" href="images/lorikeet.jpg">
	<link rel="apple-touch-icon" href="images/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">

</head>
<body>



	<!-- Primary Page Layout
	================================================== -->

	<!-- Delete everything in this .container and get started on your own site! -->

	<div class="container">
		<div class="sixteen columns">

			<h4 class="remove-bottom" style="margin-top: 40px">
Course web page for 
<strong>Statistical methods for linguistic research</strong>, at 
<a href="http://esslli2015.org/">ESSLLI 2015, Barcelona (3-14 August 2015)</a>
</h4>
<div>
	
<strong>Instructor</strong>: Shravan Vasishth<br>
Professor, <a href="http://www.ling.uni-potsdam.de/en/">Dept. of Linguistics</a><br> 
<a href="http://www.uni-potsdam.de/en/university-of-potsdam.html">University of Potsdam</a>, 14476 Potsdam <br>
Email: vasishth at uni hyphen potsdam dot de<br>
Home page: <a href="http://www.ling.uni-potsdam.de/~vasishth">http://www.ling.uni-potsdam.de/~vasishth</a><br>
<strong>Datacamp programming and co-development of homework assignments</strong>: <a href="http://www.ling.uni-potsdam.de/~engelmann/">Felix Engelmann</a>
<br><br><br>


</div>

<div class="column">



<p> 
Do you need this course? Do the test to find out. No data are recorded, and no information about you is recorded.


<div id="embed-exam-29892">Loading <a href="https://www.onlineexambuilder.com/diagnostic-test-do-you-need-this-course/exam-29892">Diagnostic test: Do you need this course?</a></div>
<script type="text/javascript">
  var QuizWorks = window.QuizWorks || [];
  QuizWorks.push(
    [document.getElementById("embed-exam-29892"), "exam", "29892", {
      autostart: false,
      width: "50%",
      height: "auto" 
    }]
  );
</script>
<script type="text/javascript" async defer src="//d134jvmqfdbkyi.cloudfront.net/script/embed.js"></script>



</p>


<h4> Teaching evaluation </h4>

<a href="http://www.ling.uni-potsdam.de/~vasishth/courses/pdfs/EvaluationESSLLI2015.pdf">Click here</a> for the teaching evaluations from the students at ESSLLI 2015.


<h4>Lecture notes for the courses (optional for ESSLLI)</h4>

You do not need to read any notes for ESSLLI; just follow the slides. These lecture notes are provided only in case you want to see the full lecture notes I use at Potsdam.<br>

<ol>
<li> Week 1 lecture notes: The informal introduction is <a href="https://github.com/vasishth/Statistics-lecture-notes-Potsdam/tree/master/IntroductoryStatistics">here</a> and 
more advanced notes on linear modeling are <a href="https://github.com/vasishth/LM">here</a></li>
<li> Week 2 lecture notes: <a href="https://github.com/vasishth/Statistics-lecture-notes-Potsdam/tree/master/AdvancedDataAnalysis">here</a></li>
</ol>

<h4>ESSLLI Schedule</h4>

<strong>Preparation needed for this course</strong>: I strongly advise students planning to take this course (week 1 or 2, or both) to work through <a href="https://www.datacamp.com/courses">the first two introductory R courses on datacamp</a>, especially if they do not know R.<br>

The two weeks are independent of each other, but I do assume in week 2 that you are familiar with frequentist statistics.<br><br>

<ul style="list-style-type:none">
<li><strong>Week 1 (Aug 3-7 2015)</strong>:<br>

<strong>Note: I have organized my slides by lecture rather than day. I may go faster or slower, depending on the class.</strong><br>

All materials (slides as pdf and Rnw files, R code, data) can be downloaded from
<a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week1">this github archive</a>. 
Below, I just link to the slides and some shinyapps.<br>

<strong>Homework</strong>: 
<a href="http://www.ling.uni-potsdam.de/~engelmann/">Felix Engelmann</a> and I have developed course exercises (automatically graded) 
that are delivered over datacamp. Do the exercises after the relevant lecture is done to sharpen your understanding. We are trying to improve on these exercises, so please send feedback on exercises to vasishth at uni-potsdam dot de.
To do the exercises, (a) first sign up <a href="https://www.datacamp.com/teams/90203cebddf9f61ad4f01832b42b10486ad8c73a/invite">here</a></strong> as a team member, and (b) then go <a href="https://www.datacamp.com/courses/statistical-methods-for-linguistic-research-foundational-ideas">here</a> to do the exercises.<br>

<strong>What we actually did on day 1</strong>: we finished the slides of lecture 1 up to slide 32.  On day 2, I will finish the slides of lecture 1, and probably get to slide 12 or so of lecture 2.<br>

<strong>What we actually did on day 2</strong>: We got to slide 20 in lecture 2.<br>

<strong>What we actually did on day 3</strong>: We got to slide 53 in lecture 2.
One question that came up was: what are degrees of freedom? Please take a look at 
<a href="http://www.nohsteachers.info/PCaso/AP_Statistics/PDFs/DegreesOfFreedom.pdf">this 
article</a>.<br>

<strong>What we actually did on day 4</strong>: We got to slide 18 in lecture 3. One question that came up was: how did we get degrees of freedom 335 in the N2data? We have 14 subjects and 24 items, hence we have 14x24=336 data points. Because we do a one sample t-test, one mean is being estimated (the mean difference between the two conditions), so we nave n-1=335 data 
points. Another question was: if we have rejected the null with p-value less than 0.05, 
don't we have evidence in favor of <strong>some</strong> alternative? 
The language we will use to describe our results 
is always in terms of rejecting the null: there is (weak/strong) evidence that 
the mean (or the differences of means) is not zero.  Think of it like this: 
if there is strong evidence against the null that mu = 0,  then you have evidence that <strong>all</strong> other values of mu are possible, including ones that your favorite theory does allow and doesn't allow. So yes, you have evidence in favor of mu not being equal to 0, but that allows for a host of possibilities that your theory almost certainly excludes, so how can we conclude that there is evidence for our <strong>specific</strong> theory?
Of course, if the sample mean has the sign consistent with your 
theory, that is your MLE, so  you can say that the mean seems to be positive (but don't forget the possibility of a Type S error!). What you cannot say is that the p-value gives you 
evidence for your <strong>specific</strong> favorite alternative.
In psychology and linguistics, researchers often make the leap that they have evidence for the <strong>specific</strong> alternative they have set up, but the logic of NHST does not allow that leap. The evidence is always with reference to the null. 
It is partly because of this bizarre situation that people like Gelman are so critical of NHST.<br>

<strong>What we actually did on day 5</strong>: We literally finished all the slides! Thanks everyone for being great students and asking excellent questions. I greatly enjoyed teaching this class.<br> 


<ol>
<li>
<strong>lecture 1</strong>: <a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week1/blob/master/lecture1.pdf">Lecture 1 slides</a>, <a href="https://vasishth.shinyapps.io/AppAreaUnderCurve">shinyapp for area under the curve</a>.<br>
Homework: do Lecture 1: Basics, see <a href="https://www.datacamp.com/courses/statistical-methods-for-linguistic-research-foundational-ideas">here</a>.<br>

Topic to be covered: very basic R usage,
       basic probability theory, 
       random variables, including jointly distributed RVs, 
       probability distributions, including bivariate distributions, 
       Maximum Likelihood Estimation, sampling distribution of mean<br>


       
 </li>
 <li>      
<strong>lecture 2</strong>: <a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week1/blob/master/lecture2.pdf">Lecture 2 slides</a>, <a href="https://vasishth.shinyapps.io/AppTypeIPower">shinyapp for type I and II error</a>.<br>
Homework: do Lecture 2: Hypothesis testing, see <a href="https://www.datacamp.com/courses/statistical-methods-for-linguistic-research-foundational-ideas">here</a>.<br>

Topic to be covered: null hypothesis significance testing, 
       t-tests,
       confidence intervals, type I error,
       type II error, 
       power, 
       type M and type S errors


 </li>
 <li>
<strong>lecture 3</strong>: <a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week1/blob/master/lecture3.pdf">Lecture 3 slides</a><br>

an introduction to (generalized) linear models<br>
Homework: do Lecture 3: linear models, see <a href="https://www.datacamp.com/courses/statistical-methods-for-linguistic-research-foundational-ideas">here</a>.<br>
</li>

 <li>
<strong>lecture 4</strong>: 
<a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week1/blob/master/lecture4.pdf">Lecture 4 slides</a><br>

an introduction to linear mixed models<br>
Homework: do Lecture 4: linear mixed models, see <a href="https://www.datacamp.com/courses/statistical-methods-for-linguistic-research-foundational-ideas">here</a>.<br>

Additional reading: <a href="http://www.sciencedirect.com/science/article/pii/S0749596X12001180">Random effects structure for confirmatory hypothesis testing: Keep it maximal, by Barr, 
Levy, Scheepers, Tily, 2013, JML</a>,
<a href="http://arxiv.org/abs/1506.04967">Parsimonious mixed models, by Bates, Kliegl, Vasishth, and Baayen, ArXiv preprint</a>.
</li>     
</li>
</ol>

<li>
<strong>Week 2 (August 10-14 2015)</strong>:<br>

<strong>Prerequisites</strong>: I am assuming you know the contents of the first week's course. If you didn't attend the course, please read the lecture notes linked at the top of this page, or look at the slides linked above.<br>

All materials (slides as pdf and Rnw files, R code, data) can be downloaded from
<a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week2">this github archive</a>.<br>

Also, for entertainment, watch <a href="https://youtu.be/pWow8Qe1snQ">Andrew Gelman</a> in A Stan is Born.
<br>

<strong>Homework</strong>: I will release exercises soon.<br>

<strong>Github repository</strong>:  <a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week2/">here</a>.

<ol>
<li>
<strong>lecture 1</strong>: See <a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week2/blob/master/lecture1.pdf">here</a>. <br>

introduction to linear models and linear mixed models,
introduction to Bayes' Theorem,
some introductory remarks on Bayes vs frequentist methods

</li>
<li>
<strong>lecture 2</strong>:  <a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week2/blob/master/lecture2.pdf">here</a>.<br>

some classical conjugate form results,
examples of simple models in JAGS using linear regression<br>

<strong>Homework</strong>: Please download these exercises: <a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week2/tree/master/exercises">click here</a>.

</li>
<li>       
<strong>lecture 3</strong>: <a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week2/blob/master/lecture3.pdf">here</a>.<br>

MCMC sampling, Gibbs sampling, evaluating model convergence and model fit,
using transformations (Box-Cox procedure)


</li>
<li>       
<strong>lecture 4</strong>: <a href="https://github.com/vasishth/ESSLLI2015Vasishth_Week2/blob/master/lecture4.pdf">here</a>.<br>

fitting linear mixed models using JAGS and Stan, using informative priors, example of meta-analysis, further reading<br>

Please see our <a href="https://github.com/dmbates/RePsychLing">RePsychLing</a> package for examples of linear mixed models, both maximal and non, and Bayesian and non.

</li> 
</ul>

Additional readings, videos, and code: 

<ol>
<li>A nice summary of Bayesian data analysis <a href="http://blog.efpsa.org/2015/08/03/bayesian-statistics-why-and-how/">here</a>.</li>
<li><a href="http://www.ling.uni-potsdam.de/~vasishth/statistics/BayesLMMs.html">Bayesian LMMs using Stan: A tutorial for psychologists, linguists, and cognitive scientists, by Sorensen, Hohenstein, and Vasishth</a> (Sven Hohenstein has joined us as co-author)</li>
<li><a href="https://github.com/bnicenboim/MPT-tutorial">Multinomial processing trees tutorial</a> by <a href="http://www.ling.uni-potsdam.de/~nicenboim/">Bruno Nicenboim</a></li>
<li>Mike Betancourt, one of the developers of Stan, on Hamiltonian Monte Carlo and Stan:
<a href="http://mlss2014.hiit.fi/mlss_files/1-hmc.pdf">HMC</a>, and 
<a href="http://mlss2014.hiit.fi/mlss_files/2-stan.pdf">Stan</a>. Also see his youtube videos: <a href="https://www.youtube.com/watch?v=xWQpEAyI5s8"> here</a> and <a href="https://www.youtube.com/watch?v=pHsuIaPbNbY">here</a>. </li>
<li>A very light historical note on Monte Carlo on youtube: <a href="https://youtu.be/TtcB1MOlNiY">here</a>.
</li>
</ol>

<h4>Github repository</h4>

You can also visit <a href="https://github.com/vasishth/">my Github repository</a> for my statistics notes. There are in various degrees of incompleteness (and my apologies: there are a few mistakes that I have yet to correct) and are heavily based on an MSc program in Statistics taught at the University of Sheffield's <a href="https://www.sheffield.ac.uk/maths">School of Mathematics and Statistics</a>; I did this MSc during 2012-2015 part-time as a distance learner. I documented my experiences in a detailed review of the MSc program  <a href="http://vasishth-statistics.blogspot.de/2015/02/getting-statistics-education-review-of.html">here</a>.  Related: before I did the MSc, I completed a one-year Graduate certificate in statistics (2011-12), this is also taught at the University of Sheffield. The review is <a href="http://vasishth-statistics.blogspot.de/2011/12/part-1-of-2-review-of-graduate.html">here</a>.

<br><br>




Comments, criticism, and requests for specific topics are most welcome (email: vasishth squiggle rz dot uni-potsdam dot de).

</p></div>

<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
var sc_project=1072336; 
var sc_invisible=1; 
var sc_security="c3e3fa93"; 
</script>
<script type="text/javascript"
src="http://www.statcounter.com/counter/counter.js"></script>
<noscript><div class="statcounter"><a title="web statistics"
href="http://statcounter.com/free-web-stats/"
target="_blank"><img class="statcounter"
src="http://c.statcounter.com/1072336/0/c3e3fa93/0/"
alt="web statistics"></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->
<!-- End Document
================================================== -->


</body>
</html>

