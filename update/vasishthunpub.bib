@comment{{This file has been generated by bib2bib 1.96}}

@comment{{Command line: /Users/shravanvasishth/bin/bib2bib -ob vasishthunpub.bib -c 'author : "Vasishth"' -c '$type="UNPUBLISHED" or $type="unpublished"' bibcleaned.bib}}

@comment{{This file has been generated by bib2bib 1.74}}

@comment{{Command line: bib2bib /sw/share/texmf/bibtex/vasishth/bib/bibliography.bib}}

@unpublished{NicenboimPreactivation2019,
  title = {Are words pre-activated probabilistically during sentence comprehension? Evidence from new data and a {B}ayesian random-effects meta-analysis using publicly available data},
  author = {Bruno Nicenboim and Shravan Vasishth and Frank R{\"o}sler},
  year = {2019},
  pdf = {https://psyarxiv.com/2atrh/},
  note = {Submitted}
}

@unpublished{EngelmannJaegerVasishthSubmitted2018,
  author = {Engelmann, Felix and J{\"a}ger, Lena A. and Vasishth, Shravan},
  note = {Manuscript submitted to Cognitive Science},
  title = {The effect of prominence and cue association in retrieval processes: {A} computational account},
  pdf = {https://osf.io/b56qv/},
  abstract = {
We present a model of cue-based retrieval in sentence processing that formalizes (i) memory accessibility (prominence) and (ii) a theory of associative cues as extensions to the ACT-R model of Lewis and Vasishth (2005). The extensions are independently motivated and, compared to the original model, enable more differentiated predictions with respect to the experimental design of individual experiments as well as differences between retrieval contexts. The predictions of the original and the extended model are compared with the results of a comprehensive Bayesian meta-analysis of published studies on retrieval interference in reflexive-/reciprocal-antecedent and subject-verb dependencies (JaÌˆger, Engelmann, Vasishth, submitted). Quantitative simulations show that the extended model accounts for effects that are outside the scope of the original model. The results emphasize the importance of accounting for different aspects of memory accessibility, for individual study design, and context-based feature-selectivity in order to generate accurate predictions of a model of cue-based memory retrieval. The simulation results thus shed new light on the cognitive mechanisms un- derlying interference effects and should be considered in the interpretation of the available data and in the design of future experiments.},
  year = {2018}
}

@comment{{BibDesk Smart Groups
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple Computer//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Vasishthpubs</string>
				<key>value</key>
				<string>vasishth</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>vasishth</string>
	</dict>
</array>
</plist>
}}

@unpublished{VasishthEngelmann2018,
  title = {Sentence comprehension as a cognitive process: {A} computational approach},
  author = {Shravan Vasishth and Felix Engelmann},
  year = {Book under contract with Cambridge University Press},
  publisher = {Cambridge University Press},
  url = {https://vasishth.github.io/sccp/}
}

@unpublished{BatesEtAlParsimonious,
  author = {Douglas Bates and Reinhold Kliegl and Shravan Vasishth and Harald Baayen},
  note = {Unpublished manuscript},
  title = {Parsimonious mixed models},
  year = {2015},
  pdf = {http://arxiv.org/abs/1506.04967},
  abstract = {The analysis of experimental data with mixed-effects models requires
decisions about the specification of the appropriate random-effects structure.
Recently, Barr, et al 2013, recommended  fitting `maximal'
models with all possible random effect components included.  Estimation of
maximal models, however, may not converge.  We show that failure to converge
 typically is not due to a suboptimal estimation algorithm, but is
a consequence of attempting to fit a model that is too complex to be properly
supported by the data, irrespective of whether estimation is based on maximum
likelihood or on Bayesian hierarchical modeling with uninformative or weakly
informative priors.  Importantly, even under convergence, overparameterization
may lead to uninterpretable models.  We provide diagnostic tools for detecting
overparameterization and guiding model simplification.  Finally, we clarify
that the simulations on which Barr et al. base their recommendations are
atypical for real data.  A detailed example is provided of how subject-related
attentional fluctuation across trials may further qualify
statistical inferences about fixed effects, and of how such nonlinear effects
can be accommodated within the mixed-effects modeling framework.}
}

@unpublished{SchadEtAlWorkflow,
  author = {Daniel J. Schad and Michael Betancourt and Shravan Vasishth},
  note = {Unpublished manuscript},
  title = {Towards a principled {B}ayesian workflow: {A} tutorial for cognitive science},
  year = {2019},
  pdf = {https://osf.io/b2vx9/}
}

@unpublished{SchadEtAlcontrasts,
  author = {Daniel J. Schad and Sven Hohenstein and Shravan Vasishth and Reinhold Kliegl},
  note = {Unpublished manuscript},
  title = {How to capitalize on a priori contrasts in linear (mixed) models: {A} tutorial},
  year = {2018},
  pdf = {https://arxiv.org/abs/1807.10451}
}

@unpublished{SchadVasishthprobnull,
  author = {Daniel J. Schad and Shravan Vasishth},
  note = {Submitted},
  title = {The posterior probability of a null hypothesis given a statistically
 significant result},
  year = {2019},
  code = {https://osf.io/9g5bp/},
  url = {https://arxiv.org/abs/1901.06889}
}

@unpublished{ALV2019,
  author = {Avetisyan, Serine and Lago, Sol and Vasishth, Shravan},
  note = {PsyArXiv preprint},
  pdf = {https://psyarxiv.com/kmbgy},
  title = {Does case marking affect agreement attraction in comprehension?},
  year = {2019}
}

@unpublished{JaegerMertzenVanDykeVasishth2018,
  author = {J\"ager, Lena A. and Mertzen, Daniela and Van Dyke, Julie A. and Vasishth, Shravan},
  url = {https://psyarxiv.com/7c4gu},
  title = {Interference patterns in subject-verb agreement and reflexives revisited: {A} large-sample study},
  year = {2019},
  note = {Submitted}
}

@unpublished{VasishthMixture2017,
  title = {Bayesian Hierarchical Finite Mixture Models of Reading Times: A Case Study},
  author = {Shravan Vasishth and Bruno Nicenboim and Nicolas Chopin and Robin Ryder},
  url = {https://osf.io/fwx3s/},
  abstract = {We present a case study demonstrating the importance of Bayesian hierarchical mixture models as a modelling tool for evaluating the predictions of competing theories of cognitive processes. As a case study, we revisit two published data sets from psycholinguistics.
In sentence comprehension, it is widely assumed that the distance between linguistic co-dependents affects the latency of dependency resolution: the longer the distance, the longer the time taken to complete the dependency (e.g., Gibson 2000). An alternative theory, direct access (McElree, 1993), assumes that retrieval times are a mixture of two distributions (Nicenboim & Vasishth, 2017): one distribution represents successful retrievals and the other represents an initial failure to retrieve the correct dependent, followed by a reanalysis that leads to successful retrieval. Here, dependency distance has the effect that in long-distance conditions the proportion of reanalyses is higher. We implement both theories as Bayesian hierarchical models and show that the direct-access model fits the Chinese relative clause reading time data better than the dependency-distance account.
This work makes several novel contributions.
First, we demonstrate how the researcher can reason about the underlying generative process of their data, thereby expressing the underlying cognitive process as a statistical model.
Second, we show how models that have been developed in an exploratory manner to represent different underlying generative processes can be compared in terms of their predictive performance, using both K-fold cross validation on existing data, and using completely new data. Finally, we show how the models can be evaluated using simulated data.},
  year = {2017},
  code = {http://www.ling.uni-potsdam.de/~vasishth/code/MixtureChinese.zip},
  note = {Unpublished MS},
  doi = {10.17605/OSF.IO/FWX3S}
}

