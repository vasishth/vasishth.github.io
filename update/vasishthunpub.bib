@comment{{This file has been generated by bib2bib 1.96}}

@comment{{Command line: /Users/shravanvasishth/bin/bib2bib -ob vasishthunpub.bib -c 'author : "Vasishth"' -c '$type="UNPUBLISHED" or $type="unpublished"' bibcleaned.bib}}

@comment{{This file has been generated by bib2bib 1.74}}

@comment{{Command line: bib2bib /sw/share/texmf/bibtex/vasishth/bib/bibliography.bib}}

@unpublished{smith2019smithvasishthfeatures,
  title = {A principled approach to feature selection in models of
sentence processing},
  author = {Smith, Garrett and Vasishth, Shravan},
  year = {2019},
  pdf = {https://psyarxiv.com/a8hju}
}

@unpublished{VasishthGelman2019,
  title = {How to embrace variation and accept uncertainty in linguistic and psycholinguistic data analysis},
  author = {Shravan Vasishth and Andrew Gelman},
  year = {2019},
  pdf = {https://psyarxiv.com/zcf8s/},
  note = {Submitted},
  optjournal = {Linguistics}
}

@comment{{BibDesk Smart Groups
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple Computer//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Vasishthpubs</string>
				<key>value</key>
				<string>vasishth</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>vasishth</string>
	</dict>
</array>
</plist>
}}

@unpublished{Rabe2019,
  author = {Maximilian M. Rabe and Johan Chandra and Andr{\'e} Kr{\"u"}gel and Stefan A. Seelig and Shravan Vasishth and Ralf Engbert},
  note = {Submitted},
  title = {A {B}ayesian Approach to Dynamical Modeling of Eye-movement Control in Reading of Normal, Mirrored, and Scrambled Texts},
  year = {2019}
}

@unpublished{LissonEtAl2020,
  author = {Paula Liss{\'o}n and Mick van het Nederend and Bruno Nicenboim and Dario Paape and Dorothea Pregla and Frank Burchert and Nicole Stadie and David Caplan and Shravan Vasishth},
  note = {preprint},
  title = {A computational evaluation of two models of retrieval processes in sentence processing: {T}he case of aphasia},
  opturl = {xxx},
  year = {2020}
}

@unpublished{BatesEtAlParsimonious,
  author = {Bates, Douglas M. and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
  note = {Unpublished manuscript},
  title = {Parsimonious mixed models},
  year = {2015},
  pdf = {http://arxiv.org/abs/1506.04967},
  abstract = {The analysis of experimental data with mixed-effects models requires
decisions about the specification of the appropriate random-effects structure.
Recently, Barr, et al 2013, recommended  fitting `maximal'
models with all possible random effect components included.  Estimation of
maximal models, however, may not converge.  We show that failure to converge
 typically is not due to a suboptimal estimation algorithm, but is
a consequence of attempting to fit a model that is too complex to be properly
supported by the data, irrespective of whether estimation is based on maximum
likelihood or on Bayesian hierarchical modeling with uninformative or weakly
informative priors.  Importantly, even under convergence, overparameterization
may lead to uninterpretable models.  We provide diagnostic tools for detecting
overparameterization and guiding model simplification.  Finally, we clarify
that the simulations on which Barr et al. base their recommendations are
atypical for real data.  A detailed example is provided of how subject-related
attentional fluctuation across trials may further qualify
statistical inferences about fixed effects, and of how such nonlinear effects
can be accommodated within the mixed-effects modeling framework.}
}

@unpublished{SchadVasishthprobnull,
  author = {Daniel J. Schad and Shravan Vasishth},
  note = {Submitted},
  title = {The posterior probability of a null hypothesis given a statistically
 significant result},
  year = {2019},
  code = {https://osf.io/9g5bp/},
  url = {https://arxiv.org/abs/1901.06889}
}

@unpublished{stone2020effect,
  title = {The effect of decay and lexical uncertainty on processing long-distance dependencies in reading},
  author = {Stone, Kate and von der Malsburg, Titus and Vasishth, Shravan},
  year = {2020},
  pdf = {https://osf.io/xrm43},
  publisher = {OSF Preprints}
}

@unpublished{VasishthMixture2017,
  title = {Bayesian Hierarchical Finite Mixture Models of Reading Times: A Case Study},
  author = {Shravan Vasishth and Bruno Nicenboim and Nicolas Chopin and Robin Ryder},
  url = {https://osf.io/fwx3s/},
  abstract = {We present a case study demonstrating the importance of Bayesian hierarchical mixture models as a modelling tool for evaluating the predictions of competing theories of cognitive processes. As a case study, we revisit two published data sets from psycholinguistics.
In sentence comprehension, it is widely assumed that the distance between linguistic co-dependents affects the latency of dependency resolution: the longer the distance, the longer the time taken to complete the dependency (e.g., Gibson 2000). An alternative theory, direct access (McElree, 1993), assumes that retrieval times are a mixture of two distributions (Nicenboim & Vasishth, 2017): one distribution represents successful retrievals and the other represents an initial failure to retrieve the correct dependent, followed by a reanalysis that leads to successful retrieval. Here, dependency distance has the effect that in long-distance conditions the proportion of reanalyses is higher. We implement both theories as Bayesian hierarchical models and show that the direct-access model fits the Chinese relative clause reading time data better than the dependency-distance account.
This work makes several novel contributions.
First, we demonstrate how the researcher can reason about the underlying generative process of their data, thereby expressing the underlying cognitive process as a statistical model.
Second, we show how models that have been developed in an exploratory manner to represent different underlying generative processes can be compared in terms of their predictive performance, using both K-fold cross validation on existing data, and using completely new data. Finally, we show how the models can be evaluated using simulated data.},
  year = {2017},
  code = {http://www.ling.uni-potsdam.de/~vasishth/code/MixtureChinese.zip},
  note = {Unpublished MS},
  doi = {10.17605/OSF.IO/FWX3S}
}

