@comment{{This file has been generated by bib2bib 1.99}}

@comment{{Command line: bib2bib -ob vasishthunpub.bib -c 'author : "Vasishth"' -c '$type="UNPUBLISHED" or $type="unpublished"' bibcleaned.bib}}

@comment{{This file has been generated by bib2bib 1.74}}

@comment{{Command line: bib2bib /sw/share/texmf/bibtex/vasishth/bib/bibliography.bib}}

@unpublished{BuerkiAlarioEtAl2020,
  title = {When words collide: {B}ayesian meta-analyses of distractor and target properties in the picture-word interference paradigm},
  author = {Audrey B{\"u}rki and Francois-Xavier Alario and Shravan Vasishth},
  year = {2020},
  code = {https://osf.io/u6k8j/},
  pdf = {https://arxiv.org/abs/2008.03972},
  note = {submitted}
}

@unpublished{yadavindiff2021,
  author = {Himanshu Yadav and Dario Paape and Garrett Smith and Brian Dillon and Shravan Vasishth},
  note = {Submitted},
  title = {Individual differences in cue-weighting in sentence comprehension: An evaluation using Approximate Bayesian Computation},
  year = {2021},
  pdf = {https://psyarxiv.com/4jdu5/}
}

@comment{{BibDesk Smart Groups
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple Computer//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<array>
	<dict>
		<key>conditions</key>
		<array>
			<dict>
				<key>comparison</key>
				<integer>2</integer>
				<key>key</key>
				<string>Vasishthpubs</string>
				<key>value</key>
				<string>vasishth</string>
				<key>version</key>
				<string>1</string>
			</dict>
		</array>
		<key>conjunction</key>
		<integer>0</integer>
		<key>group name</key>
		<string>vasishth</string>
	</dict>
</array>
</plist>
}}

@unpublished{PreglaTR,
  author = {Pregla, Dorothea and Liss{\'o}n, Paula and Vasishth, Shravan and Stadie, Nicole  and Burchert, Frank},
  note = {preprint},
  title = {Technical report: {Individual differences in visual world eye- tracking in aphasia in German}},
  url = {https://osf.io/bsnxu/?view_only=935420235a46439bb60cb574ecaaab66},
  year = {2020}
}

@unpublished{PreglaBL,
  author = {Pregla, Dorothea and Liss{\'o}n, Paula and Vasishth, Shravan and Burchert, Frank and Stadie, Nicole},
  note = {under review, Brain and Language},
  title = {Variability in sentence comprehension in aphasia in {G}erman},
  url = {https://doi.org/10.31234/osf.io/7hfpx},
  year = {2020}
}

@unpublished{BatesEtAlParsimonious,
  author = {Bates, Douglas M. and Kliegl, Reinhold and Vasishth, Shravan and Baayen, Harald},
  note = {Unpublished manuscript},
  title = {Parsimonious mixed models},
  year = {2015},
  pdf = {http://arxiv.org/abs/1506.04967},
  abstract = {The analysis of experimental data with mixed-effects models requires
decisions about the specification of the appropriate random-effects structure.
Recently, Barr, et al 2013, recommended  fitting `maximal'
models with all possible random effect components included.  Estimation of
maximal models, however, may not converge.  We show that failure to converge
 typically is not due to a suboptimal estimation algorithm, but is
a consequence of attempting to fit a model that is too complex to be properly
supported by the data, irrespective of whether estimation is based on maximum
likelihood or on Bayesian hierarchical modeling with uninformative or weakly
informative priors.  Importantly, even under convergence, overparameterization
may lead to uninterpretable models.  We provide diagnostic tools for detecting
overparameterization and guiding model simplification.  Finally, we clarify
that the simulations on which Barr et al. base their recommendations are
atypical for real data.  A detailed example is provided of how subject-related
attentional fluctuation across trials may further qualify
statistical inferences about fixed effects, and of how such nonlinear effects
can be accommodated within the mixed-effects modeling framework.}
}

@unpublished{SchadEtAlBF,
  author = {Daniel J. Schad and Bruno Nicenboim and Paul-Christian B{\"u}rkner and Michael Betancourt and Shravan Vasishth},
  title = {Workflow Techniques for the Robust Use of Bayes Factors},
  year = {2021},
  pdf = {https://arxiv.org/abs/2103.08744},
  code = {https://osf.io/y354c/},
  note = {Available from arXiv:2103.08744v2}
}

@unpublished{SchadVasishthprobnull,
  author = {Daniel J. Schad and Shravan Vasishth},
  note = {Submitted},
  title = {The posterior probability of a null hypothesis given a statistically
 significant result},
  year = {2019},
  code = {https://osf.io/9g5bp/},
  url = {https://arxiv.org/abs/1901.06889}
}

@unpublished{PaapeVasishthScanpaths,
  author = {Dario Paape and Shravan Vasishth},
  title = {Is reanalysis selective when regressions are consciously controlled?},
  year = {2021},
  pdf = {https://psyarxiv.com/gnehs/},
  code = {https://osf.io/j8cbh/}
}

@unpublished{MertzenEtAl2020,
  author = {Mertzen, Daniela and Laurinavichyute, Anna and Dillon, Brian W. and  Engbert, Ralf and Vasishth, Shravan},
  title = {Is there cross-linguistic evidence for proactive cue-based retrieval interference in sentence comprehension? {Eye-tracking data from English, German and Russian}},
  year = {2021},
  note = {submitted},
  url = {https://psyarxiv.com/t2j8v}
}

@unpublished{VasishthMixture2017,
  title = {Bayesian Hierarchical Finite Mixture Models of Reading Times: A Case Study},
  author = {Shravan Vasishth and Bruno Nicenboim and Nicolas Chopin and Robin Ryder},
  url = {https://arxiv.org/abs/1702.00564},
  abstract = {We present a case study demonstrating the importance of Bayesian hierarchical mixture models as a modelling tool for evaluating the predictions of competing theories of cognitive processes. As a case study, we revisit two published data sets from psycholinguistics.
In sentence comprehension, it is widely assumed that the distance between linguistic co-dependents affects the latency of dependency resolution: the longer the distance, the longer the time taken to complete the dependency (e.g., Gibson 2000). An alternative theory, direct access (McElree, 1993), assumes that retrieval times are a mixture of two distributions (Nicenboim & Vasishth, 2017): one distribution represents successful retrievals and the other represents an initial failure to retrieve the correct dependent, followed by a reanalysis that leads to successful retrieval. Here, dependency distance has the effect that in long-distance conditions the proportion of reanalyses is higher. We implement both theories as Bayesian hierarchical models and show that the direct-access model fits the Chinese relative clause reading time data better than the dependency-distance account.
This work makes several novel contributions.
First, we demonstrate how the researcher can reason about the underlying generative process of their data, thereby expressing the underlying cognitive process as a statistical model.
Second, we show how models that have been developed in an exploratory manner to represent different underlying generative processes can be compared in terms of their predictive performance, using both K-fold cross validation on existing data, and using completely new data. Finally, we show how the models can be evaluated using simulated data.},
  year = {2017},
  code = {http://www.ling.uni-potsdam.de/~vasishth/code/MixtureChinese.zip},
  note = {Unpublished MS},
  doi = {10.17605/OSF.IO/FWX3S}
}

